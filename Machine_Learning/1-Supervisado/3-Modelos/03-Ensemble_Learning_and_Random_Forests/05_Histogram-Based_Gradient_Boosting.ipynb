{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Capitulo 6 – Ensemble Learning and Random Forests**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como hicimos en NoteBooks anteriores, vamos a definir los tamaños de fuente por defecto para que las figuras queden más bonitas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rc('font', size=14)\n",
    "plt.rc('axes', labelsize=14, titlesize=14)\n",
    "plt.rc('legend', fontsize=14)\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y vamos a crear la carpeta `images/ensembles` (si no existe ya), y definir la función `save_fig()` que se utiliza a través de este Notebook para guardar las figuras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "IMAGES_PATH = Path() / \"images\" / \"ensembles\"\n",
    "IMAGES_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = IMAGES_PATH / f\"{fig_id}.{fig_extension}\"\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram-Based Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "def load_housing_data():\n",
    "    tarball_path = Path(\"datasets/housing.tgz\")\n",
    "    if not tarball_path.is_file():\n",
    "        Path(\"datasets\").mkdir(parents=True, exist_ok=True)\n",
    "        url = \"https://github.com/ageron/data/raw/main/housing.tgz\"\n",
    "        urllib.request.urlretrieve(url, tarball_path)\n",
    "        with tarfile.open(tarball_path) as housing_tarball:\n",
    "            housing_tarball.extractall(path=\"datasets\")\n",
    "    return pd.read_csv(Path(\"datasets/housing/housing.csv\"))\n",
    "\n",
    "housing = load_housing_data()\n",
    "\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n",
    "housing_labels = train_set[\"median_house_value\"]\n",
    "housing = train_set.drop(\"median_house_value\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las clases HGB también tienen dos características interesantes: admiten características categóricas y valores perdidos. Esto simplifica bastante el preprocesamiento. Sin embargo, las características categóricas deben representarse como enteros que van de 0 a un número inferior a max_bins. Para ello puede utilizar un OrdinalEncoder. Por ejemplo, a continuación se muestra cómo construir y entrenar un pipeline completo para el conjunto de datos de California housing presentado anteriormente:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('columntransformer',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('ordinalencoder',\n",
       "                                                  OrdinalEncoder(),\n",
       "                                                  ['ocean_proximity'])])),\n",
       "                ('histgradientboostingregressor',\n",
       "                 HistGradientBoostingRegressor(categorical_features=[0],\n",
       "                                               random_state=42))])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.preprocessing import OrdinalEncoder \n",
    "\n",
    "hgb_reg = make_pipeline(\n",
    "    make_column_transformer((OrdinalEncoder(), [\"ocean_proximity\"]),\n",
    "                            remainder=\"passthrough\"),\n",
    "    HistGradientBoostingRegressor(categorical_features=[0], random_state=42)\n",
    ")\n",
    "hgb_reg.fit(housing, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count       10.000000\n",
       "mean     47613.307194\n",
       "std       1295.422509\n",
       "min      44963.213061\n",
       "25%      47001.233485\n",
       "50%      48000.963564\n",
       "75%      48488.093243\n",
       "max      49176.368465\n",
       "dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluar las estadísticas RMSE para el modelo hgb_reg\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "hgb_rmses = -cross_val_score(hgb_reg, housing, housing_labels,\n",
    "                             scoring=\"neg_root_mean_squared_error\", cv=10)\n",
    "pd.Series(hgb_rmses).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo el proceso es tan corto como las importaciones. No hay necesidad de un imputer, escalador, o un one-hot encoder, por lo que es realmente conveniente. Tenga en cuenta que categorical_features debe establecerse en los índices de las columnas categóricas (o en una matriz booleana). Sin ningún ajuste de hiperparámetros, este modelo arroja un RMSE de unos 47.600, lo que no está nada mal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el ecosistema Python ML existen otras implementaciones optimizadas de gradient boosting: en particular, XGBoost, Cat-Boost y LightGBM. Estas bibliotecas existen desde hace varios años. Todas están especializadas en gradient boosting, sus APIs son muy similares a las de Scikit-Learn, y proporcionan muchas características adicionales, incluyendo aceleración GPU; ¡definitivamente deberías echarles un vistazo! Por otra parte, la librería TensorFlow Random Forests proporciona implementaciones optimizadas de una variedad de algoritmos de random forest, incluyendo random forest simples, extra-trees, GBRT, y varios más."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "nav_menu": {
   "height": "252px",
   "width": "333px"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
