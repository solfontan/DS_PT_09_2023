{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicios\n",
    "\n",
    "Completa los siguientes ejercicios utilizando lo que hemos aprendido hasta y los datos del directorio ejercicios/:\n",
    "\n",
    "1. Queremos ver los datos de las acciones de Facebook, Apple, Amazon, Netflix y Google (FAANG), pero nos dieron cada uno como un archivo CSV separado. Combínelos en un único archivo y guarde el dataframe de los datos FAANG como faang para el resto de los ejercicios:\n",
    "- a) Lee los archivos aapl.csv, amzn.csv, fb.csv, goog.csv y nflx.csv.\n",
    "- b) Añade una columna a cada dataframe, llamalo ticker, indicando el símbolo del ticker al que corresponde (el de Apple es AAPL, por ejemplo); así es como se busca una acción. En este caso, los nombres de los archivos son los símbolos de los teletipos.\n",
    "- c) Agréguelos en un dataframe.\n",
    "- d) Guarda el resultado en un archivo CSV llamado faang.csv.\n",
    "2. Con faang, utilice la conversión de tipos para convertir los valores de la columna de fecha en datetime y la columna de volumen en números enteros. A continuación, ordena por fecha y ticker.\n",
    "3. Encuentre las siete filas en faang con el valor más bajo para el volumen.\n",
    "4. En este momento, los datos están entre formato largo y ancho. Utilice melt() para que el formato sea completamente largo. Sugerencia: fecha y ticker son nuestras variables ID\n",
    "(identifican de forma única cada fila). Necesitamos fundir(melt) el resto para no tener columnas separadas para apertura, máximo, mínimo, cierre y volumen.\n",
    "5. Supongamos que descubrimos que el 26 de julio de 2018 hubo un fallo en cómo se registraron los datos. ¿Cómo debemos manejar esto? Tenga en cuenta que no se requiere codificación para este ejercicio.\n",
    "6. El Centro Europeo para la Prevención y el Control de las Enfermedades (ECDC) proporciona un conjunto de datos abierto sobre casos de COVID-19 denominado número diario de nuevos casos notificados de COVID-19 por país en todo el mundo (https://www.ecdc.europa.eu/en/publications-data/download-todays-data-geographicdistribution-covid-19-cases-worldwide). Este conjunto de datos se actualiza diariamente, pero utilizaremos una instantánea que contiene datos desde el 1 de enero de 2020 hasta el 18 de septiembre de 2020. Limpia y pivota los datos para que estén en formato ancho:\n",
    "- a) Lea el archivo covid19_casos.csv.\n",
    "- b) Cree una columna de fecha utilizando los datos de la columna dateRep y la función pd.to_datetime().\n",
    "- c) Establece la columna de fecha como índice y ordena el índice.\n",
    "- d) Sustituye todas las apariciones de United_States_of_America y United_Kingdom por USA y UK, respectivamente. Sugerencia: el método replace() puede ejecutarse en todo el dataframe.\n",
    "- e) Utilizando la columna countriesAndTerritories, filtre los datos de casos COVID-19 depurados hasta Argentina, Brasil, China, Colombia, India, Italia, México, Perú, Rusia, España, Turquía, Reino Unido y Estados Unidos.\n",
    "- f) Pivota los datos de modo que el índice contenga las fechas, las columnas contengan los nombres de los países y los valores sean los recuentos de casos (la columna de casos). Asegúrate de rellenar los valores NaN con 0.\n",
    "7. Para determinar los totales de casos por país de forma eficiente, necesitamos las habilidades de agregación que aprenderemos en el Capítulo 4, Agregación de Pandas DataFrames, así que los datos ECDC en el fichero covid19_casos.csv han sido agregados para nosotros y guardados en el fichero covid19_total_casos.csv. Contiene el número total de casos por país. Utilice estos datos para encontrar los 20 países con los mayores totales de casos de COVID-19. Sugerencias: al leer el archivo CSV, introduzca index_col='cases' y tenga en cuenta que será útil transponer los datos antes de aislar los países."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importamos librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combinación de DataFrames:\n",
    "\n",
    "1. Queremos ver los datos de las acciones de Facebook, Apple, Amazon, Netflix y Google (FAANG), pero nos dieron cada uno como un archivo CSV separado. Combínelos en un único archivo y guarde el dataframe de los datos FAANG como faang para el resto de los ejercicios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face = pd.read_csv('fb.csv', sep=',')\n",
    "apple = pd.read_csv('aapl.csv', sep=',')\n",
    "amazon = pd.read_csv('amzn.csv', sep=',')\n",
    "netflix = pd.read_csv('nflx.csv', sep=',')\n",
    "google = pd.read_csv('goog.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face.head()\n",
    "face.shape\n",
    "face['ticker'] = 'FB'\n",
    "face.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple.head()\n",
    "apple.shape\n",
    "apple['ticker'] = 'APPL'\n",
    "apple.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FA = pd.concat((face,apple))\n",
    "df_FA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon.head()\n",
    "amazon.shape\n",
    "amazon['ticker'] = 'AMZN'\n",
    "amazon.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix.head()\n",
    "netflix.shape\n",
    "netflix['ticker'] = 'NFLX'\n",
    "netflix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_AN = pd.concat((amazon,netflix))\n",
    "df_AN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google.head()\n",
    "google.shape\n",
    "google['ticker'] = 'GOOG'\n",
    "google.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FAAN = pd.concat((df_FA,df_AN))\n",
    "faang = pd.concat((df_FAAN,google))\n",
    "faang.to_csv('faang.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Con faang, utilice la conversión de tipos para convertir los valores de la columna de fecha en datetime y la columna de volumen en números enteros. A continuación, ordena por fecha y ticker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faang = pd.read_csv('faang.csv', sep=',')\n",
    "faang.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faang.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faang['date'] = pd.to_datetime(faang['date'])\n",
    "faang['volume'] =  faang['volume'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faang.info()\n",
    "faang.sort_values(by=['date','ticker'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faang.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Encuentre las siete filas en faang con el valor más bajo para el volumen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faang.loc[:,'volume'].sort_values(ascending=True).head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. En este momento, los datos están entre formato largo y ancho. Utilice melt() para que el formato sea completamente largo. Sugerencia: fecha y ticker son nuestras variables ID\n",
    "(identifican de forma única cada fila). Necesitamos fundir(melt) el resto para no tener columnas separadas para apertura, máximo, mínimo, cierre y volumen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faang.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faang_df = faang.melt(\n",
    "    id_vars=('date','ticker'),\n",
    "    value_vars=['high', 'low', 'open','close', 'volume']\n",
    ")\n",
    "faang_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Supongamos que descubrimos que el 26 de julio de 2018 hubo un fallo en cómo se registraron los datos. ¿Cómo debemos manejar esto? Tenga en cuenta que no se requiere codificación para este ejercicio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faang_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faang_df.query('date == \"2018-07-26\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. El Centro Europeo para la Prevención y el Control de las Enfermedades (ECDC) proporciona un conjunto de datos abierto sobre casos de COVID-19 denominado número diario de nuevos casos notificados de COVID-19 por país en todo el mundo (https://www.ecdc.europa.eu/en/publications-data/download-todays-data-geographicdistribution-covid-19-cases-worldwide). Este conjunto de datos se actualiza diariamente, pero utilizaremos una instantánea que contiene datos desde el 1 de enero de 2020 hasta el 18 de septiembre de 2020. Limpia y pivota los datos para que estén en formato ancho:\n",
    "- a) Lea el archivo covid19_casos.csv.\n",
    "- b) Cree una columna de fecha utilizando los datos de la columna dateRep y la función pd.to_datetime().\n",
    "- c) Establece la columna de fecha como índice y ordena el índice.\n",
    "- d) Sustituye todas las apariciones de United_States_of_America y United_Kingdom por USA y UK, respectivamente. Sugerencia: el método replace() puede ejecutarse en todo el dataframe.\n",
    "- e) Utilizando la columna countriesAndTerritories, filtre los datos de casos COVID-19 depurados hasta Argentina, Brasil, China, Colombia, India, Italia, México, Perú, Rusia, España, Turquía, Reino Unido y Estados Unidos.\n",
    "- f) Pivota los datos de modo que el índice contenga las fechas, las columnas contengan los nombres de los países y los valores sean los recuentos de casos (la columna de casos). Asegúrate de rellenar los valores NaN con 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_df = pd.read_csv('covid19_cases.csv')\n",
    "covid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_df['date'] = pd.to_datetime(covid_df['dateRep'], dayfirst=True, yearfirst=True)\n",
    "covid_df.set_index('date',inplace=True)\n",
    "covid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_df['countriesAndTerritories'].replace('United_States_of_America', 'USA',inplace=True)\n",
    "covid_df['countriesAndTerritories'].replace('United_Kingdom', 'UK',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_df['countriesAndTerritories'].unique() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - e) Utilizando la columna countriesAndTerritories, filtre los datos de casos COVID-19 depurados\n",
    "# hasta Argentina, Brasil, China, Colombia, India, Italia, México, Perú, Rusia, España, Turquía, Reino Unido y Estados Unidos.\n",
    "\n",
    "countries = ['Argentina', 'Brazil', 'China', 'Colombia', 'India', 'Italy', 'Mexico', 'Peru', 'Russia', 'Spain', 'Turkey', 'UK','USA']\n",
    "\n",
    "countries.groupby(countries)['popData2019'].count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
