{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Añadir y eliminar datos\n",
    "\n",
    "## Acerca de los datos\n",
    "En este cuaderno trabajaremos con datos de terremotos del 18 de septiembre de 2018 al 13 de octubre de 2018 (obtenidos del US Geological Survey (USGS) mediante la [USGS API](https://earthquake.usgs.gov/fdsnws/event/1/))\n",
    "\n",
    "## Configuración\n",
    "Estaremos trabajando con el archivo `data/earthquakes.csv` nuevamente, por lo que necesitamos manejar nuestras importaciones y leerlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\n",
    "    'data/earthquakes.csv', \n",
    "    usecols=['time', 'title', 'place', 'magType', 'mag', 'alert', 'tsunami']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de nuevos datos\n",
    "### Añadir nuevas columnas\n",
    "Las nuevas columnas se añaden a la derecha de las columnas originales y pueden ser un único valor, que será **difundido** a lo largo de las filas del marco de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['source'] = 'USGS API'\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...o una máscara booleana:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mag_negative'] = df.mag < 0\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Añadir la columna `parsed_place`\n",
    "Tenemos un problema de reconocimiento de entidades con la columna `place`. Hay varias entidades que tienen varios nombres en los datos (por ejemplo, CA y California, NV y Nevada)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.place.str.extract(r', (.*$)')[0].sort_values().unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reemplazar partes de los nombres `place` para adaptarlos a nuestras necesidades:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['parsed_place'] = df.place.str.replace(\n",
    "    r'.* of ', '', regex=True # eliminar el \"of\"\n",
    ").str.replace(\n",
    "    'the ', '' # eliminar \"the\"\n",
    ").str.replace(\n",
    "    r'CA$', 'California', regex=True # Corregir California\n",
    ").str.replace(\n",
    "    r'NV$', 'Nevada', regex=True # Corregir Nevada\n",
    ").str.replace(\n",
    "    r'MX$', 'Mexico', regex=True # Corregir Mexico\n",
    ").str.replace(\n",
    "    r' region$', '', regex=True # cortar las terminaciones con \"región\"\n",
    ").str.replace(\n",
    "    'northern ', '' # eliminar \"northern\"\n",
    ").str.replace(\n",
    "    'Fiji Islands', 'Fiji' # Alinear las plazas de Fiji\n",
    ").str.replace(\n",
    "    r'^.*, ', '', regex=True # eliminar cualquier otra cosa extraña desde el principio\n",
    ").str.strip() # eliminar los espacios sobrantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos utilizar un solo nombre para obtener todos los terremotos de ese lugar (aunque esto todavía no es perfecto):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.parsed_place.sort_values().unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilización del método `assign()` para crear columnas\n",
    "Para crear muchas columnas a la vez o actualizar columnas existentes, podemos utilizar `assign()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.assign(\n",
    "    in_ca=df.parsed_place.str.endswith('California'),\n",
    "    in_alaska=df.parsed_place.str.endswith('Alaska')\n",
    ").sample(5, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el uso de funciones `lambda`, el método `assign()` se vuelve aún más potente. **Las funciones lambda** son funciones anónimas que suelen definirse en una sola línea y para un solo uso. El método `assign()` pasa todo el marco de datos a la función `lambda` como `x`; desde ahí, podemos seleccionar las columnas `in_ca` y `in_alaska`, que se están creando en esa misma llamada a `assign()`. Aquí, utilizamos una función `lambda` para crear una nueva columna, `neither`, que indica si el terremoto no se produjo ni en Alaska ni en California:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.assign(\n",
    "    in_ca=df.parsed_place == 'California',\n",
    "    in_alaska=df.parsed_place == 'Alaska',\n",
    "    neither=lambda x: ~x.in_ca & ~x.in_alaska\n",
    ").sample(5, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenación\n",
    "Supongamos que trabajamos con dos marcos de datos distintos, uno con terremotos acompañados de tsunamis y otro con terremotos sin tsunamis. Si quisiéramos ver los terremotos en su conjunto, tendríamos que concatenar los marcos de datos en uno solo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsunami = df[df.tsunami == 1]\n",
    "no_tsunami = df[df.tsunami == 0]\n",
    "\n",
    "tsunami.shape, no_tsunami.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenar a lo largo del eje de filas (`axis=0`) equivale a añadir hasta el final. Al concatenar los terremotos con tsunami y sin tsunami, obtenemos el conjunto completo de datos de terremotos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([tsunami, no_tsunami]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que el resultado anterior es equivalente a ejecutar el método `append()` del marco de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([tsunami, no_tsunami]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos estado trabajando con un subconjunto de las columnas del fichero CSV, pero supongamos que ahora queremos obtener algunas de las columnas que ignoramos cuando leímos los datos. Como hemos añadido nuevas columnas en este cuaderno, no querremos leer el fichero y volver a realizar esas operaciones. En su lugar, concatenaremos a lo largo de las columnas (`axis=1`) para volver a añadir lo que nos falta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_columns = pd.read_csv(\n",
    "    'data/earthquakes.csv', usecols=['tz', 'felt', 'ids']\n",
    ")\n",
    "pd.concat([df.head(2), additional_columns.head(2)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero fíjate en lo que ocurre si el índice no está alineado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_columns = pd.read_csv(\n",
    "    'data/earthquakes.csv', usecols=['tz', 'felt', 'ids', 'time'], index_col='time'\n",
    ")\n",
    "pd.concat([df.head(2), additional_columns.head(2)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si el índice no está alineado, podemos alinearlo antes de intentar la concatenación, de la que hablaremos en el capítulo 3.\n",
    "\n",
    "Digamos que queremos unir los marcos de datos `tsunami` y `no_tsunami`, pero el marco de datos `no_tsunami` tiene una columna adicional. El parámetro `join` especifica cómo manejar cualquier solapamiento en los nombres de las columnas (cuando se añaden a la parte inferior) o en los nombres de las filas (cuando se concatenan a la izquierda/derecha). Por defecto, este parámetro es `outer`, por lo que conservamos todo; sin embargo, si utilizamos `inner`, sólo conservaremos lo que esté en común:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(\n",
    "    [tsunami.head(2), no_tsunami.head(2).assign(type='earthquake')], join='inner'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además, utilizamos `ignore_index`, ya que el índice no significa nada para nosotros aquí. Esto nos da valores secuenciales en lugar de lo que teníamos en el resultado anterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(\n",
    "    [tsunami.head(2), no_tsunami.head(2).assign(type='earthquake')], join='inner', ignore_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Borrado de datos no deseados\n",
    "Las columnas pueden borrarse utilizando la sintaxis de diccionario con `del`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['source']\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si no sabemos si la columna existe, debemos utilizar un bloque `try`/`except`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del df['source']\n",
    "except KeyError:\n",
    "    # handle the error here\n",
    "    print('not there anymore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos utilizar `pop()`. Esto nos permitirá utilizar las series que eliminemos más tarde. Tenga en cuenta que habrá un error si la clave no existe, por lo que también podemos utilizar un `try`/`except` aquí:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_negative = df.pop('mag_negative')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fíjate que ahora tenemos una máscara en `mag_negative`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_negative.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, podemos utilizar `mag_negative` para filtrar nuestros datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[mag_negative].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando el método `drop()`.\n",
    "Podemos eliminar filas pasando una lista de índices al método `drop()`. Observa en el siguiente ejemplo que cuando pedimos las 2 primeras filas con `head()` obtenemos la 3ª y 4ª filas porque hemos eliminado las 2 primeras originales con `drop([0, 1])`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([0, 1]).head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método `drop()` elimina por defecto a lo largo del eje de filas. Si pasamos una lista de columnas con el argumento `columns`, podemos eliminar columnas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [\n",
    "    col for col in df.columns\n",
    "    if col not in ['alert', 'mag', 'title', 'time', 'tsunami']\n",
    "]\n",
    "df.drop(columns=cols_to_drop).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También tenemos la opción de utilizar `axis=1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=cols_to_drop).equals(\n",
    "    df.drop(cols_to_drop, axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por defecto, `drop()`, junto con la mayoría de los métodos `DataFrame`, devolverá un nuevo objeto `DataFrame`. Si sólo queremos cambiar el objeto con el que estamos trabajando, podemos pasarle `inplace=True`. Esto debe usarse con cuidado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=cols_to_drop, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-between; margin-bottom: 10px;\">\n",
    "    <div style=\"text-align: left;\">\n",
    "        <a href=\"./5-subconjunto_data.ipynb\">\n",
    "            <button>&#8592; Notebook Anterior</button>\n",
    "        </a>\n",
    "    </div>\n",
    "    <div style=\"text-align: center;\">\n",
    "        <a href=\"../solutions/ch_02/solutions.ipynb\">\n",
    "            <button>Soluciones</button>\n",
    "        </a>\n",
    "    </div>\n",
    "    <div style=\"text-align: right;\">\n",
    "        <a href=\"../ch_03/1-ancho_vs_largo.ipynb\">\n",
    "            <button>Capitulo 3 &#8594;</button>\n",
    "        </a>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "<hr>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
