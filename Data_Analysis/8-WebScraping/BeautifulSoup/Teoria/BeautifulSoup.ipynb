{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beautiful Soup Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como científico de datos, tarde o temprano llegarás a un punto en el que tendrás que recopilar grandes cantidades de datos. Ya sea un proyecto o por pasatiempo y no siempre podremos contar con las API, pero tranquilo tenemos el web scraping... ¡Y una de las mejores herramientas de web scraping es Beautiful Soup!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Pero.... qué es el web scraping?\n",
    "\n",
    "En pocas palabras, el web scraping es la recopilación automatizada de datos de sitios web (para ser más precisos, del contenido HTML de los sitios web).\n",
    "\n",
    "En este Jupyter, aprenderás los conceptos básicos sobre cómo extraer datos de HTML. \n",
    "\n",
    "Lo harás extrayendo datos de la página animeseries.io, y para lograr esto, también tendrá que hacer uso de un poco de pandas principalmente.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conoce a tus nuevos mejores amigos: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Beautiful Soup\n",
    "- Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para obtener la experiencia completa de Beautiful Soup, también deberás instalar un parser, dentro de ellos tenemos..\n",
    "\n",
    "- html.parser\n",
    "- lxml\n",
    "- html5lib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a utilizar el lxml ya que es el mas rápido "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se necesita una cosa más para que podamos comenzar a hacer web scraping, y es la biblioteca de ```requests```. Con ```requests``` podemos solicitar páginas web de sitios web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora asi manos a la obra.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mi primer scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como siempre lo primero es importar las librerías "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "from string import ascii_uppercase\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, estamos listos para solicitar nuestra primera página web. No es nada complicado: guardamos la URL que queremos raspar en la variable URL, luego solicitamos la URL (requests.get (url)) y guardamos la respuesta en la variable de respuesta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://animeseries.so/popular-anime\"\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cómo saber si se guardo correctamente el sitio web?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posibles respuestas:\n",
    "\n",
    "- [Respuestas informativas](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status#information_responses) (100–199)\n",
    "- [Respuestas exitosas](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status#successful_responses) (200–299)\n",
    "- [Mensajes de redirección](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status#redirection_messages) (300–399)\n",
    "- [Respuestas de error del cliente](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status#client_error_responses) (400–499)\n",
    "- [Respuestas de error del servidor](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status#server_error_responses) (500–599)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero necesitamos el contenido HTML de la página web solicitada, así que como siguiente paso guardamos el contenido de la respuesta a html:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo podemos imprimir para ver su estructura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este es el resultado obtenido en HTML de la página, pero es realmente difícil de leer..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero para eso usamos BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cómo lo hacemos?.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un objeto BeautifulSoup llamado soup con la siguiente línea de código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bs?\n",
    "\n",
    "> from bs4 import BeautifulSoup as bs\n",
    "\n",
    "El primer parámetro del método bs() es html (que fue la variable en la que guardamos ese contenido HTML difícil de leer de la URL de los libros más vendidos)\n",
    "\n",
    "El segundo parámetro ('html.parser'), es el parser que se usa en html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a ver el cambio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cómo navegar por un objeto de Beautiful Soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTML consta de elementos como enlaces, párrafos, encabezados, bloques, etc. Estos elementos están envueltos entre etiquetas; dentro de la etiqueta de apertura y cierre se puede encontrar el contenido del elemento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](img\\html-content-web-scraping.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los elementos HTML también pueden tener atributos que contienen información adicional sobre el elemento. Los atributos se definen en las etiquetas de apertura con la siguiente sintaxis: nombre del atributo = \"valor del atributo\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](img\\attribute-example-for-web-scraping-1536x386.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que hemos aprendido algo de HTML básico, finalmente podemos comenzar a extraer datos de soup. Simplemente escriba un nombre de etiqueta después de soup y un punto (como soup.title), y observe cómo se desarrolla la magia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.h1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y sí queremos solo el texto?.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.h1.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué sucede si solo necesita el atributo de un elemento? Tampoco hay problema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.img['src']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.img.get('src')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos..\n",
    "> soup.a.get(\"href\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La sintaxis de soup.```cualquier_etiqueta``` devuelve solo el primer elemento con ese nombre de etiqueta. En lugar de soup.```cualquier_etiqueta```, también puedes usar el método .find() y obtendrás exactamente el mismo resultado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sin utilizar .find()\")\n",
    "print(soup.h1)\n",
    "print(\"Utilizando .find()\")\n",
    "print(soup.find(\"h1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A menudo, no solo necesitas uno, sino todos los elementos (por ejemplo, cada enlace en una página). Para eso es bueno el método .find_all():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si nos fijamos podemos ver que lo que nos devuelve es una lista.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qué podemos hacer con una lista?.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_a = soup.find_all('a')\n",
    "for a in all_a[2:5]:\n",
    "    print(a.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a tomar el grid principal para buscar los animes en toda la página"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = soup.find_all('div', \n",
    "            class_ = 'content_episode revent datagrild')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cantidad de paginas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "href_valor = soup.find_all('li', class_='last')[0].find('a').get('href')\n",
    "\n",
    "numero_pagina = re.search(r'page=(\\d+)', href_valor)\n",
    "\n",
    "if numero_pagina:\n",
    "    numero_pagina = numero_pagina.group(1)\n",
    "numero_pagina"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se itera sobre la lista de elementos contenidos en el grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for items in grid[:1]:\n",
    "    animes = items.find_all('div', class_ = 'name')\n",
    "    url_anime = items.find_all('a')\n",
    "    \n",
    "anime = animes[0].text\n",
    "anime_link = url_anime[0].get('href')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se obtiene el la lista de animes y urls pero no esta completa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Titulo: ', anime)\n",
    "print('--*--'*20)\n",
    "print('Link parcial: ', anime_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para construir la url completa se necesita la url base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = 'https://animeseries.so'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uniendo la url base con la obtenida anteriormente se completa la url de cada anime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_url = base + anime_link\n",
    "new_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es necesario repetir los pasos anteriores para la nueva url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea una Sopa nueva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(new_url)\n",
    "soup2 = bs(response.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estando dentro de esta nueva página se vuelve a inspeccionar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se tomara el recuadro mas externo porque es el que contiene toda la información"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_body = soup2.find_all('div', class_ = 'main_body')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la imagen necesitamos el recuadro de la izquierda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recuadro_izq = main_body[0].find_all('div', class_ = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la información el de la derecha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recuadro_der = main_body[0].find_all('div', class_ = 'right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos la url de la imagen primero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagen = recuadro_izq[0].find('img').get('src')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora con el recuadro derecho obtenemos la informacion:\n",
    "\n",
    "- Descripción\n",
    "- Otros nombres\n",
    "- Pais\n",
    "- Status\n",
    "- Fecha\n",
    "- Genero\n",
    "- Tipo\n",
    "- Temporada\n",
    "- Episodios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etiqueta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etiqueta = recuadro_der[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descripción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descripcion = etiqueta.find('p').text\n",
    "print('Descripcion: ',descripcion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otros Titulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elementos_p = etiqueta.find_all('p', class_='des')\n",
    "\n",
    "texto_deseado = elementos_p[0].get_text(separator=' ', strip=True)\n",
    "\n",
    "span_text = elementos_p[0].find('span').get_text(strip=True)\n",
    "otros_titulos = texto_deseado.replace(span_text, '', 1).strip()\n",
    "otros_titulos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elementos_p = etiqueta.find_all('p', class_='des')\n",
    "\n",
    "texto_deseado = elementos_p[1].get_text(separator=' ', strip=True)\n",
    "\n",
    "span_text = elementos_p[1].find('span').get_text(strip=True)\n",
    "pais = texto_deseado.replace(span_text, '', 1).strip()\n",
    "pais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Status, Fecha, Generos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporada = list()\n",
    "genero = list()\n",
    "for i in etiqueta.find_all('a')[2:]:\n",
    "    temporada.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, fecha = temporada[:2]\n",
    "genero = temporada[2:]\n",
    "genero = ', '.join(genero)\n",
    "\n",
    "print('Status: ',status)\n",
    "print('Fecha: ', fecha)\n",
    "print('Generos: ',genero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Episodios, Tipo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listaEpisodios = main_body[0].find_all('div',class_='list_episode')\n",
    "\n",
    "try:\n",
    "    texto = listaEpisodios[0].find('span',class_='name').text\n",
    "    texto = re.search(r'Episode [0-9]+',texto)\n",
    "    episodios = re.search('[0-9]+', texto[0])[0]\n",
    "    if int(episodios) > 0 and int(episodios) < 2:\n",
    "        tipo = 'Pelicula'\n",
    "    else:\n",
    "        tipo = 'Serie'\n",
    "except:\n",
    "    episodios = None\n",
    "    tipo = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temporada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def season_word_to_number(word):\n",
    "    word_to_number = {\n",
    "        'first': '1',\n",
    "        'second': '2',\n",
    "        'third': '3',\n",
    "        'fourth': '4',\n",
    "        'fifth': '5',\n",
    "    }\n",
    "    return word_to_number.get(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = re.search(r\"Season (\\d+)\", anime)\n",
    "if match:\n",
    "    temporada = match.group(1)\n",
    "else:\n",
    "    temporada = None\n",
    "\n",
    "if temporada is None:\n",
    "    match_descripcion = re.search(r\"(first|second|third|fourth|fifth) season\", descripcion, re.IGNORECASE)\n",
    "    if match_descripcion:\n",
    "        temporada_palabra = match_descripcion.group(1)\n",
    "        temporada = season_word_to_number(temporada_palabra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_dicc = {\n",
    "    'Titulo': anime,\n",
    "    'Alternativos': otros_titulos,\n",
    "    'Descripcion': descripcion,\n",
    "    'Status': status,\n",
    "    'Genre': genero,\n",
    "    'Tipo': tipo,  \n",
    "    'Episodes': episodios,\n",
    "    'Temporada': temporada,\n",
    "    'Fecha': fecha,\n",
    "    'Pais': pais,\n",
    "    'Imagen': imagen,\n",
    "    'URL': new_url\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([anime_dicc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teniendo todos los elementos realizamos al automatización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_dicc = {\n",
    "    'Titulo': [],\n",
    "    'Alternativos': [],\n",
    "    'Descripcion': [],\n",
    "    'Status': [],\n",
    "    'Genre': [],\n",
    "    'Tipo': [],  \n",
    "    'Episodios': [],\n",
    "    'Temporada': [],\n",
    "    'Fecha': [],\n",
    "    'Pais': [],\n",
    "    'Imagen': [],\n",
    "    'URL': []\n",
    "}\n",
    "\n",
    "base = 'https://animeseries.so'\n",
    "\n",
    "\n",
    "def season_word_to_number(word):\n",
    "    word_to_number = {\n",
    "        'first': '1',\n",
    "        'second': '2',\n",
    "        'third': '3',\n",
    "        'fourth': '4',\n",
    "        'fifth': '5',\n",
    "    }\n",
    "    return word_to_number.get(word.lower())\n",
    "\n",
    "url = f\"https://animeseries.so/popular-anime\"\n",
    "response = requests.get(url)\n",
    "soup = bs(response.content, \"html.parser\")\n",
    "\n",
    "href_valor = soup.find_all('li', class_='last')[0].find('a').get('href')\n",
    "\n",
    "numero_pagina = re.search(r'page=(\\d+)', href_valor)\n",
    "\n",
    "if numero_pagina:\n",
    "    numero_pagina = numero_pagina.group(1)\n",
    "\n",
    "# for page in range(1,int(numero_pagina)+1):\n",
    "for page in range(1,3):\n",
    "    \n",
    "    print(f\"\\rProcessing page {page}/{len(range(1, 3))}..\")\n",
    "    \n",
    "    url = f\"https://animeseries.so/popular-anime?page={page}\"\n",
    "    response = requests.get(url)\n",
    "    soup = bs(response.content, \"html.parser\")\n",
    "\n",
    "    grid = soup.find_all('div', class_ = 'content_episode revent datagrild')\n",
    "\n",
    "    animes = [anime.text for anime in grid[0].find_all('div', class_ = 'name')]\n",
    "    enlaces_filtrados = [a for a in grid[0].find_all('a') if a.has_attr('title')]\n",
    "    animes_url = [base+a.get('href') for a in enlaces_filtrados]\n",
    "\n",
    "    for ind,anime_url in enumerate(animes_url):\n",
    "        \n",
    "        print(f\"\\rProcessing {ind+1}/{len(animes_url)}..\")\n",
    "        \n",
    "        response = requests.get(anime_url)\n",
    "        soup2 = bs(response.content, \"html.parser\")\n",
    "        main_body = soup2.find_all('div', class_ = 'main_body')\n",
    "        \n",
    "        recuadro_izq = main_body[0].find_all('div', class_ = 'left')\n",
    "        imagen = recuadro_izq[0].find('img').get('src')\n",
    "        \n",
    "        recuadro_der = main_body[0].find_all('div', class_ = 'right')\n",
    "        etiqueta = recuadro_der[0]\n",
    "        descripcion = etiqueta.find('p').text\n",
    "        elementos_p = etiqueta.find_all('p', class_='des')\n",
    "        resultados = [(p.get_text(separator=' ', strip=True).replace(p.find('span').get_text(strip=True), '', 1).strip()) for p in elementos_p]\n",
    "        otros_titulos = resultados[0]\n",
    "        pais = resultados[1]\n",
    "        enlaces = etiqueta.find_all('a')[2:]\n",
    "        temporada = [i.text for i in enlaces]\n",
    "        status, fecha, *generos = temporada\n",
    "        genero = ', '.join(generos)\n",
    "        listaEpisodios = main_body[0].find_all('div',class_='list_episode')\n",
    "        try:\n",
    "            texto = listaEpisodios[0].find('span',class_='name').text\n",
    "            texto = re.search(r'Episode [0-9]+',texto)\n",
    "            episodios = re.search('[0-9]+', texto[0])[0]\n",
    "            if int(episodios) > 0 and int(episodios) < 2:\n",
    "                tipo = 'Pelicula'\n",
    "            else:\n",
    "                tipo = 'Serie'\n",
    "        except:\n",
    "            episodios = None\n",
    "            tipo = None\n",
    "        \n",
    "        anime_dicc['Titulo'].append(animes[ind])\n",
    "        anime_dicc['Alternativos'].append(otros_titulos)\n",
    "        anime_dicc['Descripcion'].append(descripcion)\n",
    "        anime_dicc['Status'].append(status)\n",
    "        anime_dicc['Genre'].append(genero)\n",
    "        anime_dicc['Tipo'].append(tipo)  \n",
    "        anime_dicc['Episodios'].append(episodios)\n",
    "        anime_dicc['Temporada'].append(temporada)\n",
    "        anime_dicc['Fecha'].append(fecha)\n",
    "        anime_dicc['Pais'].append(pais)\n",
    "        anime_dicc['Imagen'].append(imagen)\n",
    "        anime_dicc['URL'].append(anime_url)\n",
    "    \n",
    "df = pd.DataFrame(anime_dicc)\n",
    "print('\\rWeb Scraping Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_dicc = {\n",
    "    'Titulo': [],\n",
    "    'Alternativos': [],\n",
    "    'Descripcion': [],\n",
    "    'Status': [],\n",
    "    'Genre': [],\n",
    "    'Tipo': [],  \n",
    "    'Episodios': [],\n",
    "    'Temporada': [],\n",
    "    'Fecha': [],\n",
    "    'Pais': [],\n",
    "    'Imagen': [],\n",
    "    'URL': []\n",
    "}\n",
    "\n",
    "base = 'https://animeseries.so'\n",
    "\n",
    "\n",
    "def season_word_to_number(word):\n",
    "    word_to_number = {\n",
    "        'first': '1',\n",
    "        'second': '2',\n",
    "        'third': '3',\n",
    "        'fourth': '4',\n",
    "        'fifth': '5',\n",
    "    }\n",
    "    return word_to_number.get(word.lower())\n",
    "\n",
    "letras = list(ascii_uppercase)\n",
    "letras.insert(0,\"special\")\n",
    "\n",
    "# for alphabet in letras:\n",
    "for page in [letras[0]]:\n",
    "    \n",
    "    print(f\"\\rProcessing page {page}/{len([letras[0]])}..\")\n",
    "    \n",
    "    url = f\"https://animeseries.so/search/character={page}\"\n",
    "    response = requests.get(url)\n",
    "    soup = bs(response.content, \"html.parser\")\n",
    "\n",
    "    grid = soup.find_all('div', class_ = 'content_episode revent datagrild')\n",
    "\n",
    "    animes = [anime.text for anime in grid[0].find_all('div', class_ = 'name')]\n",
    "    enlaces_filtrados = [a for a in grid[0].find_all('a') if a.has_attr('title')]\n",
    "    animes_url = [base+a.get('href') for a in enlaces_filtrados]\n",
    "\n",
    "    for ind,anime_url in enumerate(animes_url):\n",
    "        \n",
    "        print(f\"\\rProcessing {ind+1}/{len(animes_url)}..\")\n",
    "        \n",
    "        response = requests.get(anime_url)\n",
    "        soup2 = bs(response.content, \"html.parser\")\n",
    "        main_body = soup2.find_all('div', class_ = 'main_body')\n",
    "        \n",
    "        recuadro_izq = main_body[0].find_all('div', class_ = 'left')\n",
    "        imagen = recuadro_izq[0].find('img').get('src')\n",
    "        \n",
    "        recuadro_der = main_body[0].find_all('div', class_ = 'right')\n",
    "        etiqueta = recuadro_der[0]\n",
    "        descripcion = etiqueta.find('p').text\n",
    "        elementos_p = etiqueta.find_all('p', class_='des')\n",
    "        resultados = [(p.get_text(separator=' ', strip=True).replace(p.find('span').get_text(strip=True), '', 1).strip()) for p in elementos_p]\n",
    "        otros_titulos = resultados[0]\n",
    "        pais = resultados[1]\n",
    "        enlaces = etiqueta.find_all('a')[2:]\n",
    "        temporada = [i.text for i in enlaces]\n",
    "        status, fecha, *generos = temporada\n",
    "        genero = ', '.join(generos)\n",
    "        listaEpisodios = main_body[0].find_all('div',class_='list_episode')\n",
    "        try:\n",
    "            texto = listaEpisodios[0].find('span',class_='name').text\n",
    "            texto = re.search(r'Episode [0-9]+',texto)\n",
    "            episodios = re.search('[0-9]+', texto[0])[0]\n",
    "            if int(episodios) > 0 and int(episodios) < 2:\n",
    "                tipo = 'Pelicula'\n",
    "            else:\n",
    "                tipo = 'Serie'\n",
    "        except:\n",
    "            episodios = None\n",
    "            tipo = None\n",
    "        \n",
    "        anime_dicc['Titulo'].append(animes[ind])\n",
    "        anime_dicc['Alternativos'].append(otros_titulos)\n",
    "        anime_dicc['Descripcion'].append(descripcion)\n",
    "        anime_dicc['Status'].append(status)\n",
    "        anime_dicc['Genre'].append(genero)\n",
    "        anime_dicc['Tipo'].append(tipo)  \n",
    "        anime_dicc['Episodios'].append(episodios)\n",
    "        anime_dicc['Temporada'].append(temporada)\n",
    "        anime_dicc['Fecha'].append(fecha)\n",
    "        anime_dicc['Pais'].append(pais)\n",
    "        anime_dicc['Imagen'].append(imagen)\n",
    "        anime_dicc['URL'].append(anime_url)\n",
    "    \n",
    "df = pd.DataFrame(anime_dicc)\n",
    "print('\\rWeb Scraping Complete')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('vips')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3014cd0240ca1f6ce0563b78067a7aa42945c9e3ac90c6ce8820ee7fdf547cd1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
